{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework Week#2 - Code Assignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOA7xR6fuNWusfdv4QwuE3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qitong323/Econ-CS-206/blob/main/Homework_Week_2_Code_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning for Fun**"
      ],
      "metadata": {
        "id": "Ikk2h3yoL6Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citation (Chicago): \n",
        "Mendes, Rodolfo. “Gym Tutorial: The Frozen Lake.” Reinforcement Learning for Fun. June 16, 2019. https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/."
      ],
      "metadata": {
        "id": "-RiysrAOL_CS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gym Tutorial: The Frozen Lake**"
      ],
      "metadata": {
        "id": "-uhRyIgMDkw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this article, we are going to learn how to create and explore the Frozen Lake environment using the [Gym library](https://gym.openai.com/), an open source project created by [OpenAI](https://openai.com/) used for reinforcement learning experiments. The Gym library defines a uniform interface for environments what makes the integration between algorithms and environment easier for developers. Among many ready-to-use environments, the default installation includes a text-mode version of the Frozen Lake game, used as example in our last post."
      ],
      "metadata": {
        "id": "hgrIqgbyD0yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Frozen Lake Environment**"
      ],
      "metadata": {
        "id": "2t4WxLNUD8F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step to create the game is to import the Gym library and create the environment. The code below shows how to do it:"
      ],
      "metadata": {
        "id": "dMFBl-cyEB3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex1.py\n",
        "import gym # loading the Gym library\n",
        " \n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()                    \n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzrjCOBrEGLU",
        "outputId": "37bbc9bd-075a-4446-f995-44af2ab209fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first instruction imports Gym objects to our current namespace. The next line calls the method *gym.make()* to create the Frozen Lake environment and then we call the method *env.reset()* to put it on its initial state. Finally, we call the method *env.render()* to print its state:\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://launchyourintelligentapphome.files.wordpress.com/2019/06/screen-shot-2019-06-14-at-23.20.33.png\" />\n",
        "</p >\n",
        "<br><i><center>Output of the the method env.render()</i></br></center>"
      ],
      "metadata": {
        "id": "BVH9gHo5EMxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the same grid we saw in the [previous post](https://reinforcement-learning4.fun/2019/06/09/introduction-reinforcement-learning-frozen-lake-example/) now is represented by a matrix of characters. Their meaning is as follows:\n",
        "\n",
        "\n",
        "\n",
        "*   **S**: initial state\n",
        "*   **F**: frozen lake\n",
        "*   **H**: hole\n",
        "*   **G**: the goal\n",
        "*   **Red square**: indicates the current position of the player\n",
        "\n",
        "Also, we can inspect the possible actions to perform in the environment, as well as the possible states of the game:"
      ],
      "metadata": {
        "id": "cx6alb-XGq-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex1.py\n",
        " \n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfcy-0ilHL1S",
        "outputId": "65a0ca99-9c91-47dd-d0ae-a27493fb5f1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space:  Discrete(4)\n",
            "Observation space:  Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we print on the console the field *action_space* and the field *observation_space*. The returned objects are of the type *Discrete*, which describes a discrete space of size n. For example, the *action_space* for the Frozen Lake environment is a discrete space of 4 values, which means that the possible values for this space are 0 (zero), 1, 2 and 3. Yet, the *observation_space* is a discrete space of 16 values, which goes from 0 to 15. Besides, these objects offer some utility methods, like the *sample()* method which returns a random value from the space. With this method, we can easily create a dummy agent that plays the game randomly:"
      ],
      "metadata": {
        "id": "4nh1orjOHPdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex2.py\n",
        "import gym\n",
        " \n",
        "MAX_ITERATIONS = 10\n",
        " \n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()\n",
        "env.render()\n",
        "for i in range(MAX_ITERATIONS):\n",
        "    random_action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(\n",
        "       random_action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8dYLd6yHduo",
        "outputId": "7f4c5581-0ad8-4d2e-b7c3-5b0e3dea6c18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above executes the game for a maximum of 10 iterations using the method *sample()* from the *action_space* object to select a random action. Then the *env.step()* method takes the action as input, executes the action on the environment and returns a tuple of four values:\n",
        "\n",
        "*   **new_state**: the new state of the environment\n",
        "*   **reward**: the reward\n",
        "*   **done**: a boolean flag indicating if the returned state is a terminal state\n",
        "*   **info**: an object with additional information for debugging purposes\n",
        "\n",
        "Finally, we use the method *env.render()* to print the grid on the console and use the returned “done” flag to break the loop. Notice that the selected action is printed together with the grid:\n",
        "<p align=\"center\">\n",
        "  <img src= https://launchyourintelligentapphome.files.wordpress.com/2019/06/screen-shot-2019-06-16-at-08.35.24.png />\n",
        "</p >\n",
        "<br><i><center>Output of successive calls to env.render() method, after selecting an action to execute</i></br></center>"
      ],
      "metadata": {
        "id": "In_M3lW2H6pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stochastic vs Deterministic**"
      ],
      "metadata": {
        "id": "GQYf2urpKfR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note in the previous output the cases in which the player moves in a different direction than the one chosen by the agent. This behavior is completely normal in the Frozen Lake environment because it simulates a slippery surface. Also, this behavior represents an important characteristic of real-world environments: the transitions from one state to another, for a given action, are probabilistic. For example, if we shoot a bow and arrow there’s a chance to hit the target as well as to miss it. The distribution between these two possibilities will depend on our skill and other factors, like the direction of the wind, for example. Due to this probabilistic nature, the final result of a state transition does not depend entirely on the taken action.\n",
        "\n",
        "By default, the Frozen Lake environment provided in Gym has probabilistic transitions between states. In other words, even when our agent chooses to move in one direction, the environment can execute a movement in another direction:"
      ],
      "metadata": {
        "id": "MR1fPztyKlY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex3.py\n",
        "import gym\n",
        " \n",
        "actions = {\n",
        "    'Left': 0,\n",
        "    'Down': 1,\n",
        "    'Right': 2, \n",
        "    'Up': 3\n",
        "}\n",
        " \n",
        "print('---- winning sequence ------ ')\n",
        "winning_sequence = (2 * ['Right']) + (3 * ['Down'])\n",
        "    + ['Right']\n",
        "print(winning_sequence)\n",
        " \n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.reset()\n",
        "env.render()\n",
        " \n",
        "for a in winning_sequence:\n",
        "    new_state, reward, done, info = env.step(actions[a])\n",
        "    print()\n",
        "    env.render()\n",
        "    print(\"Reward: {:.2f}\".format(reward))\n",
        "    print(info)\n",
        "    if done:\n",
        "        break  \n",
        " \n",
        "print()"
      ],
      "metadata": {
        "id": "wMy96rimKoEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the code above, we can observe different results and paths at each execution. Also, using the info object returned by the step method we can inspect the probability used by the environment to choose the executed movement:\n",
        "<p align=\"center\">\n",
        "  <img src= https://launchyourintelligentapphome.files.wordpress.com/2019/06/screen-shot-2019-06-16-at-17.09.04.png?w=808&h=808>\n",
        "</p >\n",
        "<br><i><center>The character moved in directions other than the selected one, with probability of 0.3333…</i></br></center>"
      ],
      "metadata": {
        "id": "tYeX9PYBKsqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the Frozen Lake environment can also be used in deterministic mode. By setting the property *is_slippery=False* when creating the environment, the slippery surface is turned off and then the environment always executes the action chosen by the agent:"
      ],
      "metadata": {
        "id": "uMXtiXjvK5ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex4.py\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False)"
      ],
      "metadata": {
        "id": "p6IprzTNK8tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the probabilities returned in the info object is always equals to 1.0.\n",
        "<p align=\"center\">\n",
        "  <img src= https://launchyourintelligentapphome.files.wordpress.com/2019/06/screen-shot-2019-06-16-at-17.16.35.png?w=578&h=1174>\n",
        "</p >\n",
        "<br><i><center>In deterministic mode, the agent always move in the selected direction</i></br></center>"
      ],
      "metadata": {
        "id": "oh-mIqYLLBLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Map sizes and custom maps**"
      ],
      "metadata": {
        "id": "I54OuRazLNcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default 4×4 map is not the only option to play the Frozen Lake game. Also, there’s an 8×8 version that we can create in two different ways. The first one is to use the specific environment id for the 8×8 map:"
      ],
      "metadata": {
        "id": "sV5qBxxVLPiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex5.py\n",
        "env = gym.make(\"FrozenLake8x8-v0\")\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOzTuw3WLVHS",
        "outputId": "50759ac3-7738-4986-8698-b1e75f8300e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second option is to call the make method passing the value “8×8” as an argument to the map_name parameter:"
      ],
      "metadata": {
        "id": "2uVBLFZDLZT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen-lake-ex5.py\n",
        "env = gym.make('FrozenLake-v0', map_name='8x8')\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mSEWRbnLbeb",
        "outputId": "a21ceee2-a6be-436b-ddf7-ede2c39817fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, we can create our custom map of the Frozen Lake game by passing an array of strings representing the map as an argument to the parameter desc:"
      ],
      "metadata": {
        "id": "7K77QAMkLdgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_map = [\n",
        "    'SFFHF',\n",
        "    'HFHFF',\n",
        "    'HFFFH',\n",
        "    'HHHFH',\n",
        "    'HFFFG'\n",
        "]\n",
        " \n",
        "env = gym.make('FrozenLake-v0', desc=custom_map)\n",
        "env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZviSObEJLfSi",
        "outputId": "ae344294-4b84-443a-9a34-5e684de6a163"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFHF\n",
            "HFHFF\n",
            "HFFFH\n",
            "HHHFH\n",
            "HFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "2KxcbkvELhqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this post, we learned how to use the Gym library to create an environment to train a reinforcement learning agent. We focused on the Frozen Lake environment, a text mode game with simple rules but that allows us to explore the fundamental concepts of reinforcement learning."
      ],
      "metadata": {
        "id": "WI7F0xgHLkHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**"
      ],
      "metadata": {
        "id": "l6vNoRGkLmii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A brief introduction to reinforcement learning concepts can be found at [How AI Learns to Play Games](https://reinforcement-learning4.fun/2019/06/03/how-ai-learns-play-games/). The Frozen Lake game rules and fundamental concepts of reinforcement learning can be found at [Introduction to Reinforcement Learning: the Frozen Lake Example](https://reinforcement-learning4.fun/2019/06/09/introduction-reinforcement-learning-frozen-lake-example/). Finally, you find instructions on how to install the Gym environment, check the post [How to Install Gym](https://reinforcement-learning4.fun/2019/05/24/how-to-install-openai-gym/).\n",
        "\n",
        "Finally, the code examples for this post can be found at https://github.com/rodmsmendes/reinforcementlearning4fun/tree/master/gym-tutorial-frozen-lake.\n",
        "\n"
      ],
      "metadata": {
        "id": "9om--9XULuBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Answers to Research Questions**\n",
        "\n",
        "*   **Research Question (RQ) 1**: How to experiment with the Frozen-AI environment in openAI gym by the dichotomy of environment and agents?  (2 points)\n",
        "*   **Research Question (RQ) 2**: What are the general principles and references for lucid communication by professional markdowns and code formatting? (2 points)\n",
        "\n",
        "*   **Research Question (RQ) 3**: How to tell stories using the Frozen-AI environment in openAI gym with lucid communication by professional markdowns and code formatting in Jupyter Notebook?  (2 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22uo4BegRDc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   To begin with, we should import the Gym library and create the Frozen Lake on its initial-state Environment, which is represented by a matrix of characters with different meanings. After inspecting the possible actions as well as states in the game, we can then execute the game for certain iterations and use the returned “done” flag to break the loop. Notice that there would be difference in the direction between the player moves in and the agent chooses, which is concluded as “stochastic” action and attributes to its “simulation of a slippery surface”. That lead to different results and paths at each execution. To avoid the possibility of “stochastic” action, we can use Frozen Lake environment in deterministic mode by setting the property *is_slippery=Fase* when creating the environment. Thus, the probabilities returned in the info object is always equal to 1.0, implying that the agent always move in the selected direction.\n",
        "\n",
        "2.   What are the general principles and references for lucid communication by professional markdowns and code formatting? (2 points)\n",
        "Both code cells and markdown cells are incorporated to allow developers to annotate their code and visualizations with mathematical expressions and other structures to keep track of code developments and logic. \n",
        "The first and foremost principle for lucid communication is standard styling. Perhaps the simplest element of markdown is the heading. The number symbol # is used to indicate various sizes of headings. Although italics and bolding are offered by most markdown editors, they’re easy to write out with asterisks (* for italics, ** for bolding). Using block quotes can be used to quote something from a text or something said by someone. This is done with a right arrow >. Multiple right arrows can be used to form nested block quotes. Besides, we need to tell the story in a standard format with a smooth logic. That includes dividing the whole article into different sections and listing out the idea ina specific order (with numbers or bullet points). \n",
        "In terms of code formatting, we can import code when necessary to further demonstrate ideas and improve the interactions. Using proper layout helps identify parts of code that belong together, are dependent on one another, and more. Another part of making code readable is to optimize its content. Here, it is important that what you write makes sense and people can understand what each element, function or variable does. Additionally, we should involve code comments to clarify the markups, such that chances are good that whenever readers looks at the work, they will be able to understand what is going on quite quickly. \n",
        "\n",
        "3. The article can be a great example to tell the whole story. Firstly, it should have a clear logic flow, mainly on how to build the environment and then how to create the game. The article starts with creating the Frozen Lake environment and develops step by step according to the algorithm towards rhe conclusion. That offers readers an acceptable learning curve when reading the article and makes it easier to digest abstract concepts by explaining one by one. Secondly, the article has a good structure with different sub-sections. Having got diverse sub-titles for each section, the article successfully improves the readability by breaking a giant goal into small pieces and leaves readers with several small tasks to complete. Thirdly, it illustrates code, hyperlinks and generated diagrams to make the story more vivid and understandable. That provides readers chances to have a try themselves or to check the results to have a better insight in their concept and given code. The hyperlink also makes the further readings more accessible to readers and improves the efficiency. "
      ],
      "metadata": {
        "id": "4CNTkZmgStiT"
      }
    }
  ]
}